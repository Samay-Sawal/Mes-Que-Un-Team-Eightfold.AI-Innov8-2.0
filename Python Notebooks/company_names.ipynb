{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PwK_AqRBCMsk"
      },
      "outputs": [],
      "source": [
        "# Importing necessary dependencies\n",
        "import os\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Yv1PmLEK-_",
        "outputId": "36a16e9a-ab48-462f-ffce-d5191e643100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU for inference.\n"
          ]
        }
      ],
      "source": [
        "# Checking if the GPU is available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using {'GPU' if device == 0 else 'CPU'} for inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO79ReKERXzk"
      },
      "source": [
        "In order to get the company names which is basically the companies/institutes where an applicant has been. We will be using a modified BERT model for this purpose. The specific modification used here is **BERT Named Entity Recognition**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc308SnVCuj3",
        "outputId": "1bdf2cca-96e1-494f-82de-f470595a93f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Loading the necessary models\n",
        "# Loading spaCy's pre-trained NER model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Loading a BERT-based NER pipeline from Hugging Face's transformers\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SItTX1CMC6lg"
      },
      "outputs": [],
      "source": [
        "# Reading the previously parsed resumes\n",
        "input_csv_file = 'parsed_resumes.csv'\n",
        "df = pd.read_csv(input_csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jY0mLdzaDI5E"
      },
      "outputs": [],
      "source": [
        "# Function to extract company names including educational institutions\n",
        "def extract_company_names(resume_text):\n",
        "    \"\"\"Extract company names from resume text using a combination of BERT and spaCy NER.\"\"\"\n",
        "    companies = []\n",
        "\n",
        "    # Joining the list of resume lines into a single string\n",
        "    resume_full_text = \"\\n\".join(resume_text)\n",
        "\n",
        "    # Using spaCy's NER model to extract named entities\n",
        "    doc = nlp(resume_full_text)\n",
        "\n",
        "    # Filtering out organizations recognized by spaCy's model\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'ORG':  # 'ORG' refers to Organizations (potentially companies)\n",
        "            companies.append(ent.text)\n",
        "\n",
        "    # Now using the BERT NER model to extract entities\n",
        "    ner_results = ner_pipeline(resume_full_text)\n",
        "\n",
        "    # Filtering the BERT NER results for organizations (labeled as 'ORG')\n",
        "    for entity in ner_results:\n",
        "        if entity['entity_group'] == 'ORG':\n",
        "            companies.append(entity['word'])\n",
        "\n",
        "    # Remove duplicates\n",
        "    companies = list(set(companies))\n",
        "\n",
        "    return companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svk4dHDvDbf6"
      },
      "outputs": [],
      "source": [
        "# Iterate over the resumes and extract company names for each applicant\n",
        "company_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    applicant_id = row['Applicant_ID']\n",
        "    resume_text = eval(row['Parsed_Resume'])  # Convert the string list back to an actual list of lines\n",
        "    company_names = extract_company_names(resume_text)\n",
        "\n",
        "    company_data.append([applicant_id, company_names])\n",
        "\n",
        "# Create a DataFrame for the extracted company names\n",
        "company_df = pd.DataFrame(company_data, columns=['Applicant_ID', 'Company_Names'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rwHs24kDc6e",
        "outputId": "4b3c1ba6-606b-4ca3-efcf-3ad54ee86b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company data has been successfully written to applicant_companies.csv\n"
          ]
        }
      ],
      "source": [
        "# Output CSV with applicant IDs and their respective company names\n",
        "output_csv_file = 'applicant_companies.csv'\n",
        "company_df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(f\"Company data has been successfully written to {output_csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iA9cEFBFnz5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
